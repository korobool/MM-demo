{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_low(inp_list):\n",
    "    return [x.lower() for x in inp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(inp_list, bert_list):\n",
    "    dif = list(set(inp_list).difference(bert_list))\n",
    "    for x in dif:\n",
    "        for n, y in enumerate(inp_list):\n",
    "            if x == y:\n",
    "                inp_list[n] = '[UNK]'\n",
    "    return inp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'united', 'states', 'of', 'america', 'is', 'a', 'federal', 'republic', 'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(', 'washington', ',', 'd.c.', ',', 'the', 'capital', 'city', 'of', 'the', 'united', 'states', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various', 'minor', 'islands', '.', 'the', '48', 'contiguous', 'states', 'and', 'washington', ',', 'd.c.', ',', 'are', 'in', 'central', 'north', 'america', 'between', 'canada', 'and', 'mexico', ';', 'the', 'two', 'other', 'states', ',', 'alaska', 'and', 'hawaii', ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'north', 'america', 'and', 'an', 'archipelago', 'in', 'the', 'mid', '-', 'pacific', ',', 'respectively', ',', 'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'pacific', 'ocean', 'and', 'the', 'caribbean', 'sea', '.'] \n",
      "\n",
      "['the', 'united', 'states', 'of', 'america', 'is', 'a', 'federal', 'republic', 'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(', 'washington', ',', 'd', '.', 'c', '.', ',', 'the', 'capital', 'city', 'of', 'the', 'united', 'states', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various', 'minor', 'islands', '.', 'the', '48', 'contiguous', 'states', 'and', 'washington', ',', 'd', '.', 'c', '.', ',', 'are', 'in', 'central', 'north', 'america', 'between', 'canada', 'and', 'mexico', ';', 'the', 'two', 'other', 'states', ',', 'alaska', 'and', 'hawaii', ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'north', 'america', 'and', 'an', 'archipelago', 'in', 'the', 'mid', '-', 'pacific', ',', 'respectively', ',', 'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'pacific', 'ocean', 'and', 'the', 'caribbean', 'sea', '.'] \n",
      "\n",
      "['the', 'united', 'states', 'of', 'america', 'is', 'a', 'federal', 'republic', 'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(', 'washington', ',', '[UNK]', ',', 'the', 'capital', 'city', 'of', 'the', 'united', 'states', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various', 'minor', 'islands', '.', 'the', '48', 'contiguous', 'states', 'and', 'washington', ',', '[UNK]', ',', 'are', 'in', 'central', 'north', 'america', 'between', 'canada', 'and', 'mexico', ';', 'the', 'two', 'other', 'states', ',', 'alaska', 'and', 'hawaii', ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'north', 'america', 'and', 'an', 'archipelago', 'in', 'the', 'mid', '-', 'pacific', ',', 'respectively', ',', 'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'pacific', 'ocean', 'and', 'the', 'caribbean', 'sea', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenized input\n",
    "text = \"\"\"The United States of America is a federal republic consisting of 50 states,\n",
    "a federal district (Washington, D.C., the capital city of the United States),\n",
    "five major territories, and various minor islands. The 48 contiguous states and Washington,\n",
    "D.C., are in central North America between Canada and Mexico; the two other states, Alaska and Hawaii,\n",
    "are in the northwestern part of North America and an archipelago in the mid-Pacific, respectively,\n",
    "while the territories are scattered throughout the Pacific Ocean and the Caribbean Sea.\"\"\"\n",
    "\n",
    "tokenized_text_bert = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "tokenized_text_inp = ['The', 'United', 'States', 'of', 'America', 'is', 'a', 'federal', 'republic',\n",
    "                      'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(',\n",
    "                      'Washington', ',', 'D.C.', ',', 'the', 'capital', 'city', 'of', 'the', 'United',\n",
    "                      'States', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various',\n",
    "                      'minor', 'islands', '.', 'The', '48', 'contiguous', 'states', 'and', 'Washington',\n",
    "                      ',', 'D.C.', ',', 'are', 'in', 'central', 'North', 'America', 'between', 'Canada',\n",
    "                      'and', 'Mexico', ';', 'the', 'two', 'other', 'states', ',', 'Alaska', 'and', 'Hawaii',\n",
    "                      ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'North', 'America', 'and',\n",
    "                      'an', 'archipelago', 'in', 'the', 'mid', '-', 'Pacific', ',', 'respectively', ',',\n",
    "                      'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'Pacific',\n",
    "                      'Ocean', 'and', 'the', 'Caribbean', 'Sea', '.']\n",
    "\n",
    "tokenized_text_inp = to_low(tokenized_text_inp)\n",
    "print(tokenized_text_inp, '\\n')\n",
    "print(tokenized_text_bert, '\\n')\n",
    "\n",
    "tokenized_text = merge(tokenized_text_inp, tokenized_text_bert)\n",
    "print(tokenized_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_list = [(0, 4), (11, 11), (18, 18), (20, 20), (26, 28), (31, 31), (41, 41), (45, 45),\n",
    "            (47, 47), (52, 53), (55, 55), (57, 57), (60, 60), (64, 64), (66, 66), (74, 75),\n",
    "            (80, 83), (93, 95), (97, 99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the united states of america  ||| weight---> 0.01\n",
      "50  ||| weight---> 0.01\n",
      "washington  ||| weight---> 0.01\n",
      "[UNK]  ||| weight---> 0.0\n",
      "the united states  ||| weight---> 0.01\n",
      "five  ||| weight---> 0.01\n",
      "48  ||| weight---> 0.09999999999999999\n",
      "washington  ||| weight---> 0.0\n",
      "[UNK]  ||| weight---> 0.0\n",
      "north america  ||| weight---> 0.0\n",
      "canada  ||| weight---> 0.01\n",
      "mexico  ||| weight---> 0.03\n",
      "two  ||| weight---> 0.0\n",
      "alaska  ||| weight---> 0.0\n",
      "hawaii  ||| weight---> 0.0\n",
      "north america  ||| weight---> 0.0\n",
      "the mid - pacific  ||| weight---> 0.060000000000000005\n",
      "the pacific ocean  ||| weight---> 0.01\n",
      "the caribbean sea  ||| weight---> 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def bert_estimate(tokenized_text, ner_list):\n",
    "    for n in ner_list:\n",
    "        weight_factor_sum = 0\n",
    "        for i in range(n[0], n[1]+1):\n",
    "            weight_factor = 0\n",
    "            tokenized_text_copy = copy.copy(tokenized_text)\n",
    "            tokenized_text_copy[i] = '[MASK]'\n",
    "            # print(tokenized_text_copy)\n",
    "            # print(tokenized_text_copy)\n",
    "            # Convert token to vocabulary indices\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text_copy)\n",
    "\n",
    "            # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "            segments_ids = [0]*len(tokenized_text_copy)\n",
    "\n",
    "            # Convert inputs to PyTorch tensors\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "            # Predict all tokens\n",
    "            predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            item, pred = torch.sort(predictions[0, i], descending=True)\n",
    "            predicted_tokens = []\n",
    "            for index in pred[:50]:\n",
    "                predicted_tokens.append(tokenizer.convert_ids_to_tokens([index.item()])[0])\n",
    "            for one_token in predicted_tokens:\n",
    "                if one_token != tokenized_text[i]:\n",
    "                    weight_factor += 0.02\n",
    "                else:\n",
    "                    break\n",
    "            weight_factor_sum += weight_factor\n",
    "        weight_factor_sum /= len(n)\n",
    "        print(' '.join(tokenized_text[n[0]: n[1]+1]), ' ||| weight--->', weight_factor_sum)\n",
    "\n",
    "bert_estimate(tokenized_text, ner_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
