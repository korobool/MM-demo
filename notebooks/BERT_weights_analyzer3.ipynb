{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import math\n",
    "import copy\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_low(inp_list):\n",
    "    return [x.lower() for x in inp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(inp_list, bert_list):\n",
    "    dif = list(set(inp_list).difference(bert_list))\n",
    "    for x in dif:\n",
    "        for n, y in enumerate(inp_list):\n",
    "            if x == y:\n",
    "                inp_list[n] = '[UNK]'\n",
    "    return inp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'united', 'states', 'of', 'america', 'is', 'a', 'federal', 'republic', 'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(', 'washington', ',', 'd.c.', ',', 'the', 'capital', 'city', 'of', 'the', 'united', 'states', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various', 'minor', 'islands', '.', 'the', '48', 'contiguous', 'states', 'and', 'washington', ',', 'd.c.', ',', 'are', 'in', 'central', 'north', 'america', 'between', 'canada', 'and', 'mexico', ';', 'the', 'two', 'other', 'states', ',', 'alaska', 'and', 'hawaii', ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'north', 'america', 'and', 'an', 'archipelago', 'in', 'the', 'mid', '-', 'pacific', ',', 'respectively', ',', 'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'pacific', 'ocean', 'and', 'the', 'caribbean', 'sea', '.'] \n",
      "\n",
      "['the', 'united', 'states', 'of', 'america', 'is', 'a', 'federal', 'republic', 'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(', 'washington', ',', 'd', '.', 'c', '.', ',', 'the', 'capital', 'city', 'of', 'the', 'united', 'states', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various', 'minor', 'islands', '.', 'the', '48', 'contiguous', 'states', 'and', 'washington', ',', 'd', '.', 'c', '.', ',', 'are', 'in', 'central', 'north', 'america', 'between', 'canada', 'and', 'mexico', ';', 'the', 'two', 'other', 'states', ',', 'alaska', 'and', 'hawaii', ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'north', 'america', 'and', 'an', 'archipelago', 'in', 'the', 'mid', '-', 'pacific', ',', 'respectively', ',', 'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'pacific', 'ocean', 'and', 'the', 'caribbean', 'sea', '.'] \n",
      "\n",
      "['the', 'united', 'states', 'of', 'america', 'is', 'a', 'federal', 'republic', 'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(', 'washington', ',', '[UNK]', ',', 'the', 'capital', 'city', 'of', 'the', 'united', 'states', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various', 'minor', 'islands', '.', 'the', '48', 'contiguous', 'states', 'and', 'washington', ',', '[UNK]', ',', 'are', 'in', 'central', 'north', 'america', 'between', 'canada', 'and', 'mexico', ';', 'the', 'two', 'other', 'states', ',', 'alaska', 'and', 'hawaii', ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'north', 'america', 'and', 'an', 'archipelago', 'in', 'the', 'mid', '-', 'pacific', ',', 'respectively', ',', 'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'pacific', 'ocean', 'and', 'the', 'caribbean', 'sea', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenized input\n",
    "text = \"\"\"The United States of America is a federal republic consisting of 50 states,\n",
    "a federal district (Washington, D.C., the capital city of the United States),\n",
    "five major territories, and various minor islands. The 48 contiguous states and Washington,\n",
    "D.C., are in central North America between Canada and Mexico; the two other states, Alaska and Hawaii,\n",
    "are in the northwestern part of North America and an archipelago in the mid-Pacific, respectively,\n",
    "while the territories are scattered throughout the Pacific Ocean and the Caribbean Sea.\"\"\"\n",
    "\n",
    "tokenized_text_bert = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "tokenized_text_inp = ['The', 'United', 'States', 'of', 'America', 'is', 'a', 'federal', 'republic',\n",
    "                      'consisting', 'of', '50', 'states', ',', 'a', 'federal', 'district', '(',\n",
    "                      'Washington', ',', 'D.C.', ',', 'the', 'capital', 'city', 'of', 'the', 'United',\n",
    "                      'States', ')', ',', 'five', 'major', 'territories', ',', 'and', 'various',\n",
    "                      'minor', 'islands', '.', 'The', '48', 'contiguous', 'states', 'and', 'Washington',\n",
    "                      ',', 'D.C.', ',', 'are', 'in', 'central', 'North', 'America', 'between', 'Canada',\n",
    "                      'and', 'Mexico', ';', 'the', 'two', 'other', 'states', ',', 'Alaska', 'and', 'Hawaii',\n",
    "                      ',', 'are', 'in', 'the', 'northwestern', 'part', 'of', 'North', 'America', 'and',\n",
    "                      'an', 'archipelago', 'in', 'the', 'mid', '-', 'Pacific', ',', 'respectively', ',',\n",
    "                      'while', 'the', 'territories', 'are', 'scattered', 'throughout', 'the', 'Pacific',\n",
    "                      'Ocean', 'and', 'the', 'Caribbean', 'Sea', '.']\n",
    "\n",
    "tokenized_text_inp = to_low(tokenized_text_inp)\n",
    "print(tokenized_text_inp, '\\n')\n",
    "print(tokenized_text_bert, '\\n')\n",
    "\n",
    "tokenized_text = merge(tokenized_text_inp, tokenized_text_bert)\n",
    "print(tokenized_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_list = [(0, 4), (11, 11), (18, 18), (20, 20), (26, 28), (31, 31), (41, 41), (45, 45),\n",
    "            (47, 47), (52, 53), (55, 55), (57, 57), (60, 60), (64, 64), (66, 66), (74, 75),\n",
    "            (80, 83), (93, 95), (97, 99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_weight_of_phrase(tokenized_text, ner_list):\n",
    "    results = []\n",
    "    for n in ner_list:\n",
    "        weight_factor_sum = 0\n",
    "        for i in range(n[0], n[1]+1):\n",
    "            weight_factor = 100\n",
    "            tokenized_text_copy = copy.copy(tokenized_text)\n",
    "            tokenized_text_copy[i] = '[MASK]'\n",
    "            # print(tokenized_text_copy)\n",
    "            # print(tokenized_text_copy)\n",
    "            # Convert token to vocabulary indices\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text_copy)\n",
    "\n",
    "            # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "            segments_ids = [0]*len(tokenized_text_copy)\n",
    "\n",
    "            # Convert inputs to PyTorch tensors\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "            # Predict all tokens\n",
    "            predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            item, pred = torch.sort(predictions[0, i], descending=True)\n",
    "            predicted_tokens = []\n",
    "            for index in pred[:100]:\n",
    "                predicted_tokens.append(tokenizer.convert_ids_to_tokens([index.item()])[0])\n",
    "            for one_token in predicted_tokens:\n",
    "                if one_token != tokenized_text[i]:\n",
    "                    weight_factor -= 1\n",
    "                else:\n",
    "                    break\n",
    "            weight_factor_sum += weight_factor/100\n",
    "        weight_factor_sum /= (n[1] - n[0] + 1)\n",
    "        results.append([' '.join(tokenized_text[n[0]: n[1]+1]), weight_factor_sum])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_weight_of_position(tokenized_text, ner_list):\n",
    "    results = []\n",
    "    for n in ner_list:\n",
    "        weight_factor_sum = 0\n",
    "        actual_range = [n[0], n[1]]\n",
    "        if n[0] != 0:\n",
    "            actual_range.insert(0, n[0]-1)\n",
    "        if n[1] != len(ner_list):\n",
    "            actual_range.append(n[0]+1)\n",
    "        for i in actual_range:\n",
    "            weight_factor = 100\n",
    "            tokenized_text_copy = copy.copy(tokenized_text)\n",
    "            tokenized_text_copy[i] = '[MASK]'\n",
    "            # print(tokenized_text_copy)\n",
    "            # print(tokenized_text_copy)\n",
    "            # Convert token to vocabulary indices\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text_copy)\n",
    "\n",
    "            # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "            segments_ids = [0]*len(tokenized_text_copy)\n",
    "\n",
    "            # Convert inputs to PyTorch tensors\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "            # Predict all tokens\n",
    "            predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            item, pred = torch.sort(predictions[0, i], descending=True)\n",
    "            predicted_tokens = []\n",
    "            for index in pred[:100]:\n",
    "                predicted_tokens.append(tokenizer.convert_ids_to_tokens([index.item()])[0])\n",
    "            for one_token in predicted_tokens:\n",
    "                if one_token != tokenized_text[i]:\n",
    "                    weight_factor -= 1\n",
    "                else:\n",
    "                    break\n",
    "            weight_factor_sum += weight_factor/100\n",
    "        weight_factor_sum /= len(actual_range)\n",
    "        results.append([' '.join(tokenized_text[n[0]: n[1]+1]), weight_factor_sum])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weighing(weight_of_phrase, weight_of_position, k=0.5, m=0.5):\n",
    "    results = []\n",
    "    for one_entity_1, one_entity_2 in zip(weight_of_phrase, weight_of_position):\n",
    "        weight = (one_entity_1[1]*k + one_entity_2[1]*m)/(k + m)\n",
    "        results.append([one_entity_1[0], weight])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the united states of america', 0.998], ['50', 0.99], ['washington', 0.99], ['[UNK]', 1.0], ['the united states', 0.9966666666666667], ['five', 0.99], ['48', 0.9], ['washington', 1.0], ['[UNK]', 1.0], ['north america', 1.0], ['canada', 0.99], ['mexico', 0.97], ['two', 1.0], ['alaska', 1.0], ['hawaii', 1.0], ['north america', 1.0], ['the mid - pacific', 0.985], ['the pacific ocean', 0.9966666666666667], ['the caribbean sea', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "weight_of_phrase = bert_weight_of_phrase(tokenized_text, ner_list)\n",
    "print(weight_of_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the united states of america', 0.9966666666666667], ['50', 0.995], ['washington', 0.995], ['[UNK]', 1.0], ['the united states', 0.9975], ['five', 0.9275], ['48', 0.9199999999999999], ['washington', 0.9924999999999999], ['[UNK]', 0.9975], ['north america', 1.0], ['canada', 0.9924999999999999], ['mexico', 0.9824999999999999], ['two', 0.995], ['alaska', 1.0], ['hawaii', 1.0], ['north america', 1.0], ['the mid - pacific', 0.985], ['the pacific ocean', 0.9924999999999999], ['the caribbean sea', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "weight_of_position = bert_weight_of_position(tokenized_text, ner_list)\n",
    "print(weight_of_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the united states of america', 0.9973333333333334], ['50', 0.9924999999999999], ['washington', 0.9924999999999999], ['[UNK]', 1.0], ['the united states', 0.9970833333333333], ['five', 0.95875], ['48', 0.9099999999999999], ['washington', 0.99625], ['[UNK]', 0.99875], ['north america', 1.0], ['canada', 0.99125], ['mexico', 0.97625], ['two', 0.9975], ['alaska', 1.0], ['hawaii', 1.0], ['north america', 1.0], ['the mid - pacific', 0.985], ['the pacific ocean', 0.9945833333333334], ['the caribbean sea', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "weight_average = average_weighing(weight_of_phrase, weight_of_position)\n",
    "print(weight_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorizer(data):\n",
    "    result = []\n",
    "    for x in data:\n",
    "        result.append([x[0], round((math.pow(101, x[1]) - 1), 1)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the united states of america', 99.1], ['50', 95.4], ['washington', 95.4], ['[UNK]', 100.0], ['the united states', 98.5], ['five', 95.4], ['48', 62.7], ['washington', 100.0], ['[UNK]', 100.0], ['north america', 100.0], ['canada', 95.4], ['mexico', 86.9], ['two', 100.0], ['alaska', 100.0], ['hawaii', 100.0], ['north america', 100.0], ['the mid - pacific', 93.2], ['the pacific ocean', 98.5], ['the caribbean sea', 100.0]]\n"
     ]
    }
   ],
   "source": [
    "weight_of_phrase_factorized = factorizer(weight_of_phrase)\n",
    "print(weight_of_phrase_factorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the united states of america', 98.5], ['50', 97.7], ['washington', 97.7], ['[UNK]', 100.0], ['the united states', 98.8], ['five', 71.3], ['48', 68.8], ['washington', 96.6], ['[UNK]', 98.8], ['north america', 100.0], ['canada', 96.6], ['mexico', 92.2], ['two', 97.7], ['alaska', 100.0], ['hawaii', 100.0], ['north america', 100.0], ['the mid - pacific', 93.2], ['the pacific ocean', 96.6], ['the caribbean sea', 100.0]]\n"
     ]
    }
   ],
   "source": [
    "weight_of_position_factorized = factorizer(weight_of_position)\n",
    "print(weight_of_position_factorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the united states of america', 98.8], ['50', 96.6], ['washington', 96.6], ['[UNK]', 100.0], ['the united states', 98.6], ['five', 82.5], ['48', 65.7], ['washington', 98.3], ['[UNK]', 99.4], ['north america', 100.0], ['canada', 96.0], ['mexico', 89.5], ['two', 98.8], ['alaska', 100.0], ['hawaii', 100.0], ['north america', 100.0], ['the mid - pacific', 93.2], ['the pacific ocean', 97.5], ['the caribbean sea', 100.0]]\n"
     ]
    }
   ],
   "source": [
    "weight_average_factorized = factorizer(weight_average)\n",
    "print(weight_average_factorized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
